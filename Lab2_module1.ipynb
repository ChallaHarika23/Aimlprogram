{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChallaHarika23/Aimlprogram/blob/main/Lab2_module1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecuSgeE8Wioh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqSg_hp6XLx0",
        "outputId": "8a195b41-30f5-4187-fbb6-2b92761b16fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-0c77f8e533aa>:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int)\n"
          ]
        }
      ],
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " dataset.target = dataset.target.astype(np.int)\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFIxI2pfXa-H"
      },
      "outputs": [],
      "source": [
        "def NN1(trdata, trlabel, query):\n",
        "  dif  = trdata - query\n",
        "  squ = dif*dif\n",
        "  dist = squ.sum(1)\n",
        "  label = trlabel[np.argmin(dist)]\n",
        "  return label\n",
        "\n",
        "def NN(trdata, trlabel, testdata):\n",
        "  predlabel = np.array([NN1(trdata, trlabel, i) for i in testdata])\n",
        "  return predlabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW3rIRUSXcDO"
      },
      "outputs": [],
      "source": [
        "def RandomClassifier(trdata, trlabel, testdata):\n",
        "\n",
        "\n",
        "  classes = np.unique(trlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX-qAimPYtrU"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the gtlabels and predlabels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum()\n",
        "  return correct/len(gtlabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jC7grDzBZb6u"
      },
      "outputs": [],
      "source": [
        "def split(data, label, percent):\n",
        "  rn = rng.random(len(label))\n",
        "  split1 = rn<percent\n",
        "  split2 = rn>=percent\n",
        "  split1_data = data[split1,:]\n",
        "  split1_label = label[split1]\n",
        "  split2_data = data[split2,:]\n",
        "  split2_label = label[split2]\n",
        "  return split1_data, split1_label, split2_data, split2_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dzk4ViYZLYW",
        "outputId": "1a5ee718-d8a1-4a04-b411-e2d687592f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of test samples =  6235\n",
            "Number of train samples =  14405\n",
            "Percent of testdata =  30.208333333333332 %\n"
          ]
        }
      ],
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 30/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of train samples = ', len(alltrainlabel))\n",
        "print('Percent of testdata = ', len(testlabel)*100/len(dataset.target),'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVdYI7uIXL5n"
      },
      "outputs": [],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 70/100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cZZAoAeaVe-",
        "outputId": "b533488e-4fbd-4036-eab7-4ff069ecf0ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained accuracy with nearset neighbour  1.0\n",
            "Trained accuracy using random classifier  0.16201727390052617\n"
          ]
        }
      ],
      "source": [
        "train_pred = NN(traindata, trainlabel, traindata)\n",
        "tr_Accuracy = Accuracy(trainlabel, train_pred)\n",
        "print(\"Trained accuracy with nearset neighbour \", tr_Accuracy)\n",
        "\n",
        "train_pred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "tr_Accuracy = Accuracy(trainlabel, train_pred)\n",
        "print(\"Trained accuracy using random classifier \", tr_Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWQyjqNEaVhD",
        "outputId": "ce3bfab2-125f-414f-a09c-123fb85a56e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy using nearest neighbour 0.3208679593721145\n",
            "Validation accuracy using random classifier 0.1680517082179132\n"
          ]
        }
      ],
      "source": [
        "val_pred = NN(traindata, trainlabel, valdata)\n",
        "val_Accuracy = Accuracy(vallabel, val_pred)\n",
        "print(\"Validation Accuracy using nearest neighbour\", val_Accuracy)\n",
        "\n",
        "val_pred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "val_Accuracy = Accuracy(vallabel, val_pred)\n",
        "print(\"Validation accuracy using random classifier\", val_Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNT3zt1zaVkp",
        "outputId": "f6a9f786-372a-4c64-f459-d2c931997106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy with random splits 0.34198364735158193\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 80/100)\n",
        "val_pred = NN(traindata, trainlabel, valdata)\n",
        "val_Accuracy = Accuracy(vallabel, val_pred)\n",
        "print(\"Validation accuracy with random splits\", val_Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UVTDGGWbfHH",
        "outputId": "b120c65a-d4d9-4662-dfc7-0910d66b3276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy is  0.35140336808340017\n"
          ]
        }
      ],
      "source": [
        "test_pred = NN(alltraindata, alltrainlabel, testdata)\n",
        "test_Accuracy = Accuracy(testlabel, test_pred)\n",
        "print('Test accuracy is ', test_Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U2T4ghWcKJ3",
        "outputId": "0c29b1c3-aa12-49fb-a77f-108c93c73187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy using random classifier  0.15878107457898957\n"
          ]
        }
      ],
      "source": [
        "test_pred = RandomClassifier(alltraindata, alltrainlabel, testdata)\n",
        "test_Accuracy = Accuracy(testlabel, test_pred)\n",
        "print(\"Test accuracy using random classifier \", test_Accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYGh2eICdS_q"
      },
      "source": [
        "Try it out for yourself and answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPJshaKHddIS"
      },
      "source": [
        "1.How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "\n",
        "The accuracy increases when we increase the validation set percentage.As we reduce the validation percentage accuracy decreases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_clx3-0dceB",
        "outputId": "d6e83889-8153-46c6-9bf5-22f23c3d8e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy is  0.3328581610833927\n",
            "Validation accuracy is  0.3339391185824625\n",
            "Validation accuracy with random splits 0.3225371120107962\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 90/100)\n",
        "val_pred = NN(traindata, trainlabel, valdata)\n",
        "val_Accuracy = Accuracy(vallabel, val_pred)\n",
        "print(\"Validation accuracy is \", val_Accuracy)\n",
        "\n",
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 85/100)\n",
        "val_pred = NN(traindata, trainlabel, valdata)\n",
        "val_Accuracy = Accuracy(vallabel, val_pred)\n",
        "print(\"Validation accuracy is \", val_Accuracy)\n",
        "\n",
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 80/100)\n",
        "val_pred = NN(traindata, trainlabel, valdata)\n",
        "val_Accuracy = Accuracy(vallabel, val_pred)\n",
        "print(\"Validation accuracy with random splits\", val_Accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uSCS9msV39o"
      },
      "source": [
        "2.How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "\n",
        "\n",
        "The size of the training and validation sets can have a significant impact on how well you can predict the accuracy on the test set using the validation set.\n",
        "When you have a larger training set, your model has more data to learn from, which can help it capture complex patterns in the data. This typically results in a model with lower bias (better fit to the training data).\n",
        "However, if the validation set remains small, it may not be representative of the entire dataset, leading to overfitting. In this case, your model may perform exceptionally well on the validation set but poorly on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYF230zmVtYp",
        "outputId": "2817efb7-6f3e-4a6d-c012-e909bb366b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy is  0.35140336808340017\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 80/100)\n",
        "test_pred = NN(alltraindata, alltrainlabel, testdata)\n",
        "test_Accuracy = Accuracy(testlabel, test_pred)\n",
        "print('Test accuracy is ', test_Accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spDN84f9Xpe1"
      },
      "source": [
        "3.What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "The percentage of data to reserve for the validation set is a critical decision in machine learning, and there is no one-size-fits-all answer. The appropriate size of the validation set depends on several factors, including the total size of your dataset, its complexity, and the goals of your machine learning project.\n",
        "\n",
        "However, a common practice is to use a split like 70-30 or 80-20 for training-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhOgHIVxc2uJ"
      },
      "source": [
        "### Multiple Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey6a51yycKLo"
      },
      "outputs": [],
      "source": [
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for i in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lcOzKCBczzn",
        "outputId": "24213606-00bc-4681-a8f1-25f8d7b39ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average validation accuracy is  0.3382136574289653\n",
            "test accuracy is  0.35140336808340017\n"
          ]
        }
      ],
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 80/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVO0Wz3OZHby"
      },
      "source": [
        "1.Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "\n",
        "Yes, averaging the validation accuracy across multiple splits can give more consistent and reliable results compared to relying on a single split of your data. This technique is commonly known as k-fold cross-validation, and it helps mitigate the potential impact of the specific random split of your data on the performance evaluation of your machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16jc7avxcz2E",
        "outputId": "5fc0d2c2-1a27-44fc-f5f7-892c2bd661ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is 28.448455397431452\n",
            "Test accuracy is 28.917401764234164\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def kFoldCrossValidation(alldata, alllabel, k, classifier=NN):\n",
        "    num_samples = len(alldata)\n",
        "    fold_size = num_samples // k\n",
        "    accuracies = []\n",
        "\n",
        "    for i in range(k):\n",
        "        start = i * fold_size\n",
        "        end = (i + 1) * fold_size\n",
        "\n",
        "        valdata = alldata[start:end]\n",
        "        vallabel = alllabel[start:end]\n",
        "\n",
        "        traindata = np.concatenate([alldata[:start], alldata[end:]])\n",
        "        trainlabel = np.concatenate([alllabel[:start], alllabel[end:]])\n",
        "\n",
        "        valpred = classifier(traindata, trainlabel, valdata)\n",
        "        accuracy = Accuracy(vallabel, valpred)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    return np.mean(accuracies)\n",
        "\n",
        "\n",
        "iterations = 10\n",
        "k = 5\n",
        "average_accuracy = kFoldCrossValidation(alltraindata, alltrainlabel, k, classifier=NN)\n",
        "print('Average validation accuracy is', average_accuracy)\n",
        "\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "test_accuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is', test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZjgQq-7Z7U9"
      },
      "source": [
        "2.Does it give more accurate estimate of test accuracy?\n",
        "\n",
        "k-fold cross-validation provides a more accurate estimate of test accuracy compared to a single validation split, it is still an estimate and may not always perfectly predict the test set's performance. It helps reduce bias and gives you a more reliable assessment of your model's generalization, but it does not replace the final evaluation on the actual test set, which provides the most accurate measure of how well your model will perform in real-world scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8abJYxOaU9g"
      },
      "source": [
        "3.What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "\n",
        "The number of iterations in k-fold cross-validation does have an impact on the reliability of the estimate. In general, increasing the number of iterations (folds) tends to provide a more stable and better estimate of the model's performance. However, it also comes at the cost of increased computational time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdDgm9aicz5g",
        "outputId": "a1d87ba8-ed9e-43df-c901-095407da038f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy with 5 iterations is 0.30774036792780285\n",
            "Average validation accuracy with 10 iterations is 0.30774036792780285\n",
            "Average validation accuracy with 15 iterations is 0.30774036792780285\n",
            "Average validation accuracy with 20 iterations is 0.30774036792780285\n",
            "Test accuracy is 0.35140336808340017\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def kFoldCrossValidation(alldata, alllabel, k, iterations, classifier=NN):\n",
        "    num_samples = len(alldata)\n",
        "    fold_size = num_samples // k\n",
        "    accuracies = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        fold_accuracies = []\n",
        "\n",
        "        for i in range(k):\n",
        "            start = i * fold_size\n",
        "            end = (i + 1) * fold_size\n",
        "\n",
        "            valdata = alldata[start:end]\n",
        "            vallabel = alllabel[start:end]\n",
        "\n",
        "            traindata = np.concatenate([alldata[:start], alldata[end:]])\n",
        "            trainlabel = np.concatenate([alllabel[:start], alllabel[end:]])\n",
        "\n",
        "            valpred = classifier(traindata, trainlabel, valdata)\n",
        "            accuracy = Accuracy(vallabel, valpred)\n",
        "            fold_accuracies.append(accuracy)\n",
        "\n",
        "        mean_accuracy = np.mean(fold_accuracies)\n",
        "        accuracies.append(mean_accuracy)\n",
        "\n",
        "    return np.mean(accuracies)\n",
        "\n",
        "iterations_list = [5, 10, 15, 20]\n",
        "k = 5\n",
        "\n",
        "for num_iterations in iterations_list:\n",
        "    average_accuracy = kFoldCrossValidation(alltraindata, alltrainlabel, k, num_iterations, classifier=NN)\n",
        "    print(f'Average validation accuracy with {num_iterations} iterations is', average_accuracy)\n",
        "\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "test_accuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is', test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n",
        "\n",
        "Increasing the number of iterations in cross-validation can help mitigate the impact of having a very small training or validation dataset. However, it's important to note that while more iterations can improve the stability of your performance estimate, they won't fundamentally change the limitations imposed by small datasets"
      ],
      "metadata": {
        "id": "ji-Ls25hjAAn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fnbAbjoocKPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c9ecf3-5df3-4dd1-c48c-c3a12ce8bc44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy with 5 iterations is 28.852481777160712\n",
            "Average validation accuracy with 10 iterations is 28.680319333564732\n",
            "Average validation accuracy with 15 iterations is 28.839986115931968\n",
            "Average validation accuracy with 20 iterations is 28.81603609857688\n",
            "Test accuracy is 28.163592622293503\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def kFoldCrossValidation(alldata, alllabel, k, iterations, classifier=NN):\n",
        "    num_samples = len(alldata)\n",
        "    fold_size = num_samples // k\n",
        "    accuracies = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        fold_accuracies = []\n",
        "\n",
        "        for i in range(k):\n",
        "            start = i * fold_size\n",
        "            end = (i + 1) * fold_size\n",
        "\n",
        "            valdata = alldata[start:end]\n",
        "            vallabel = alllabel[start:end]\n",
        "\n",
        "            traindata = np.concatenate([alldata[:start], alldata[end:]])\n",
        "            trainlabel = np.concatenate([alllabel[:start], alllabel[end:]])\n",
        "\n",
        "            valpred = classifier(traindata, trainlabel, valdata)\n",
        "            accuracy = Accuracy(vallabel, valpred)\n",
        "            fold_accuracies.append(accuracy)\n",
        "\n",
        "        mean_accuracy = np.mean(fold_accuracies)\n",
        "        accuracies.append(mean_accuracy)\n",
        "\n",
        "    return np.mean(accuracies)\n",
        "\n",
        "def NN(traindata, trainlabel, valdata):\n",
        "    valpred = np.random.randint(0, 2, size=len(valdata))\n",
        "    return valpred\n",
        "\n",
        "def Accuracy(true_labels, predicted_labels):\n",
        "    correct_predictions = np.sum(true_labels == predicted_labels)\n",
        "    total_samples = len(true_labels)\n",
        "    accuracy = (correct_predictions / total_samples) * 100.0\n",
        "    return accuracy\n",
        "\n",
        "k = 5\n",
        "small_train_validation_data = True\n",
        "iterations_list = [5, 10, 15, 20] if small_train_validation_data else [1]\n",
        "\n",
        "for num_iterations in iterations_list:\n",
        "    average_accuracy = kFoldCrossValidation(alltraindata, alltrainlabel, k, num_iterations, classifier=NN)\n",
        "    print(f'Average validation accuracy with {num_iterations} iterations is', average_accuracy)\n",
        "\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "test_accuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is', test_accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZvscUnVgpQeu2k3rfjHSm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}